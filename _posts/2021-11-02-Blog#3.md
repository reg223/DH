---
layout: post
title:  "Blog #3: NEC, Jekyll"
date:   2021-11-02 13:15:25 -0500 
categories: blogs
---

2021/10/01

### Notes for NEC(Named Entity Classification): (From the reading [Recent Named Entity Recognition and Classification techniques: A systematic review][NERC techniques] by Goyal et. al)

Named entity ~= class of words/ short phrases with a similar identity(?), essentially adding tags to word entities.

>A Named Entity is a word form that recognizes the elements having similar properties from a collection of elements"

Example : a set named 'GEO' with words ("Egypt","Rome")



### Application of NERC with Machine learning

As does the example, the task of NERC is simple for a human being, even without the relevant knowledge. The challenge is to train computers to perform the recognition and classification, which provides several benefits:

- *much* faster
- makes multi-language communications more feasible
- ...why not when have the (computational) power?

When training the algorithms, there are several major challenges:

*Nested Entities*: entities in entities. 

*Text Ambiguity*: When the text could have numerous interpretations. One example would be when the same term is used for several different elements (eg. Constantine, Alexandria); This is essentially the same issue when you receive a completely unexpected result from a google search.

*Training Data Annotation*: The training data for Algorithms can be biased/incorrect. For example, if an algorithm was trained with Wall Street Journal and is used to perform NERC on the *Decline and Fall*, some terms could have never showed up and others might have different definitions.

*Lack of Material*: Of course, the "easy" solution to the problem above is to simply add more training data of the kind into the model. The reality is, however, many fields (including Classics/Humanties) lack such annotated resources. The situation is worse with other languages.

### Evaluation of work

There is a quantified standard to measure how successful does a trained model perform the NERC : the f-score. There are 3 relevant variables, TP, FP, and FN.

>Precision, Recall and F-score are calculated on the basis of true positives (TP), false positives (FP) and false negatives (FN). True positives are the correctly labeled instances. False positives are the incorrectly labeled instances and false negatives are the missed out instances by the system. F-score is the weighted mean of Precision and Recall. 


![Precision, Recall, and f-score](/DH/Resources/B3F1.png "Precision, Recall, and f-score")

From this equation provided it can be derived that:

    Precision = TP/(TP+FP)
    Recall = TP/(TP+FN)

Or as this picture describes:

![Precision & Recall](/DH/Resources/B3F2.png "Precision & Recall")

## Exercise #1

<u>Objective:</u> to manually examine [chunk of Gibbon text][text] and derive the f-score.

A total of 32 entities was identified, in which the model captured:

- 12 TP
- 17 FN
- 3 FP

This gives a Precision of 80% , Recall of 41.4% and a f-score of 53.5%, which is fairly low. One odd thing about the machine's recognition it that some neglected NEs that were identical to one more more of the identified NEs. For example the name Gallienus was spotted twice and tagged correctly but was also neglected once. This performance was however expected since the model was trained with modern newspaper, while Gibbon wrote the *Decline and Fall* in 18th century. However this could not account for neglected NEs that has the same modern meaning, such as "Italy" and "Milan".







[text]:https://github.com/pnadelofficial/FallDHCourseMaterials/blob/main/Entities%20by%20Chapter/chapter11NER.txt
[NERC techniques]:https://www.sciencedirect.com/science/article/pii/S1574013717302782?via%3Dihub