---
layout: post
title:  "Blog #4: Topic modeling"
date:   2021-11-03 13:15:25 -0500 
categories: blogs
---
2021/10/14
## Prerequisite: TF-IDF  

The term stands for "Term Frequency" and "Inverse Document Frequency", which are two different measures：

![TF-IDF](/DH/Resources/B4F1.png "TF-IDF")

Using this equation, we can turn Corpus into vectors, in which every unique word/punctuation(or not) is a dimension of its own. This way we can "align" different bodies of text and compare them with each other under the same standard, since they are all vectors of the same length.

*<u> Codes used: </u>* (From [Topic Model Notebook for 10/6][Topic Model Notebook for 10/6])

    # imports

    import pandas as pd
    import pprint
    from sklearn.feature_extraction.text import TfidfVectorizer #this does the work!
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.decomposition import NMF
    from sklearn.decomposition import LatentDirichletAllocation
    from bs4 import BeautifulSoup

## Topic Modeling: the actual thing

Now that we can turn docs into vectors, there are several algorithms that can help us extract messages from them.

### Introducing NMF (Non-negative Matrix Factorization)

Provided with the Matrices (# of words * # of docs), NMF can calculate and select a demanded number of words each for a nemanded number of topics, hence extract meaning from a massive chunk of text in short amount of time.

## Excersise #2

[Here's][colab] my work.

For analysis I looked up chapter summary from [this link][summary] and compared that with results from creating 15 and 10 topics with the NMF model.

 

It was only after looking more thoroughly at the html when I realized that the creator of this file did not make any differentiation between volumes and chapters, and all volumes (or “book”) was made an empty, individual “chapter” div of their own. Likewise, the introduction was also treated as an individual chapter. Interestingly, they did seem to fall into similar categories when their topics were computed: Book 4and 5 were in the same category for both 10 and 15 topics,  the intro and book 2 also shared the same topics in both scenarios.

 

For the first actual chapter (labeled as chapter 03 in the dataframe), introducing and discussing the division of labor and it’s relationship with productivity, 10 and 15 topics yielded almost identical results , only differing with the words “workmen” (10) and “quantity” (15) and their sequence  (the full list is provided bellow) .

 

    10/8

    ['workmen', 'business', 'tools', 'employed', 'accumulation', 'great', 'society', 'industry', 'number', 'produce', 'division', 'different', 'work', 'stock', 'labour']

    

    15/8

    ['quantity', 'business', 'employed', 'tools', 'great', 'industry', 'accumulation', 'society', 'number', 'produce', 'division', 'different', 'work', 'stock', 'labour']

 

These results were able to reflect a vague idea of the actual topic of discussion, as the program did manage to pick out important elements in the text. However, unlike most chapters in Gibbon’s The Decline and Fall of Roman Empire, this chapter has an abstract, generalized topic as opposed to a specific course of events as do the former. As a result, while the key words generated were accurate enough to act as summary of the ideas in the text (bottom-up) for both books, it is extremely difficult to extract any meaningful interpretations without the text (Top-down) for Wealth of Nation since the connection between the key words were not established. This is a limitation that cannot be presented while working with Gibbon, since key words in Decline and Fall are often time and place specific.

 

From that I concluded the key difference between the two text is that Wealth of Nations have a more compatible set of ideas throughout its entire book. This was also shown in the topics as they were more of a combination set of the same larger pool of words. In this case, it is almost impossible to derive meaning from looking at individual words, and their relationship is far more important, making a NMF model without phrases much less useful.



[Topic Model Notebook for 10/6]:https://colab.research.google.com/drive/1G8utcCpXXw0Ovub8Pz7M5lRA3m2xSeVf?usp=sharing
[colab]:https://colab.research.google.com/drive/1wtDXGYdBTbowzfYEupJi-BCjHUFdMCag?usp=sharing 
[summary]:https://www.coursehero.com/lit/The-Wealth-of-Nations/volume-1-book-1-chapter-1-summary/