---
layout: post
title:  "Blog #7: Linguistics & Style II"
date:   2021-11-06 13:15:25 -0500 
categories: blogs
---

### Gibbon timeline

![GibbonTime](/DH/Resources/B7F1.png "Gibbon Timeline")

By plotting the timeline for the citations (via notes) in *Decline and Fall*, we can see a consistent trend and a change that occurred starting chapter 46: Prior to this point, the sources appeared to have a linear slowly rising trend and the time of which suggested that they are first hand sources; the points then spread out much more vertically after 46, and the slope was clearly steeper, also with much more contemporary works cited.

    Guess: In the later volumes of Decline and Fall, Gibbon is going more analytical, therefore more secondary sources used


## Gibbon's linguistic style and its progression

One advantage of DH over the traditional approaches to texts like Gibbon is that findings can be based on solid statistics rather than pure intuition, since with the former it is practically impossible to manually do things like count the use of certain words across a 1.5 million words text like *Decline and Fall*. Examples of traditional analysis found [here][GD].

One thing found was Gibbon's shift of focus in his course of writing the *Decline and Fall*, where he discussed less about individual figures in the later chapters of the book.

...How can we examine this better in DH?

### Introducing Stanza

Stanza works similarly like [spaCy][spaCy]. It extracts the grammatical nature of words in a given corpus, in units of sentences. More about abbreviations [here][penn]. The most useful feature given being the [Universal Dependencies][UD], which is listed as "deprel" in the list. This allows us to pick out simple word relationships (like "big cat") with close to perfect accuracy.

### Examples

See [this][colab] and [this][colab1] colab for examples of analysis comparing Gibbon's use of language with tat of Hume's.


## Exercise #4

The interpretation of the confusion matrix is quite simple: all items were identified as a feature of gibbon. The answer can also be easily deducted if we look into the training data: there were 21 text files representing Bryce, file size average to approximately 40 kb per file; Gibbon's training data, on the other hand, has 71 files with average file size that's 2 or even 3 times greater than training data of Bryce's - thats nearly a 1 to 10 ratio of words. **There is simply way too much Gibbon's text to form a balanced training data.** This is also why the test with the training set could not reveal the problem: it shares the problem of "Bryce deficiency".

My assumption is that the solution would be to remove some of gibbon's text until the two has similar size. Alternatively, reducing Gibbon to the same number of files could also work. I therefore tested both.

For T1 (same # of chapters) I used Gibbon's chapter 49 to 71, and 1 to 14 for T2 (similar size). Here are the results

    T1:

    Test train score: 0.4444444444444444

    RW Test result:
        '[Lausanne], July 30th, 1753. DEAR SIR, I must beg ...' => bryce
        'That there should be little community of ideas, an...' => bryce

    Confusion matrix:
        array([[5, 0],
               [0, 4]])

    T2:

    Test train score:0.5714285714285714

    RW Test result:
        '[Lausanne], July 30th, 1753. DEAR SIR, I must beg ...' => bryce
        'That there should be little community of ideas, an...' => bryce

    Confusion matrix:
        array([[0, 3],
               [0, 4]])
               
For some reasons T2's results resembled the original test, which ma potentially be a coincidence due to the overall lacking of training data, provided the accuracy. T1, though also made the wrong prediction, has better results in the  matrix. It may still be the case that we would need to add more data to Bryce in order to get a more convincing and accurate model.


[GD]:https://docs.google.com/document/d/1n_wO6f0kJ6rOqGkUhNgQdP_2OFTCZs1fHGPNTkTAEaI/edit#
[spaCy]:https://spacy.io/
[penn]:https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html
[UD]:https://universaldependencies.org/en/dep/index.html
[colab]:https://colab.research.google.com/drive/1McJeN1kz8OeZmSJlaL55Inl622WlvvyB?usp=sharing#scrollTo=nSpiFv5ixm-g
[colab1]:https://colab.research.google.com/drive/1AknxXFSsX_m9TEuR0cgUHJUyTt94Q3eg